###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401,F821
# flake8: noqa: E501,F401,F821
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "chat.baml": "class MyUserMessage {\n  role \"user\" | \"assistant\"\n  content string\n}\n\nfunction ChatWithLLM(messages: MyUserMessage[]) -> string {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Answer the user's questions based on the chat history:\n    {% for message in messages %}\n      {{ _.role(message.role) }} \n      {{ message.content }}\n    {% endfor %}\n\n    Answer:\n  \"#\n}\n\n\ntest TestName {\n  functions [ChatWithLLM]\n  args {\n    messages [\n      {\n        role \"assistant\"\n        content \"Hello!\"\n      }\n      {\n        role \"user\"\n        content \"Hello!\"\n      }\n    ]\n  }\n}\n\n",
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> CustomGPT4o {\n  provider openai\n  options {\n    model \"gpt-4o\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomGPT4oMini {\n  provider openai\n  retry_policy Exponential\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n  }\n}\n\nclient<llm> CustomSonnet {\n  provider anthropic\n  options {\n    model \"claude-3-5-sonnet-20241022\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n\nclient<llm> CustomHaiku {\n  provider anthropic\n  retry_policy Constant\n  options {\n    model \"claude-3-haiku-20240307\"\n    api_key env.ANTHROPIC_API_KEY\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/round-robin\nclient<llm> CustomFast {\n  provider round-robin\n  options {\n    // This will alternate between the two clients\n    strategy [CustomGPT4oMini, CustomHaiku]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/fallback\nclient<llm> OpenaiFallback {\n  provider fallback\n  options {\n    // This will try the clients in order until one succeeds\n    strategy [CustomGPT4oMini, CustomGPT4oMini]\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "debate.baml": "class DebateMessage {\n  role \"panelist_a\" | \"panelist_b\" | \"panelist_c\" | \"moderator\"\n  content string\n  speaker_name string @description(\"Name of the AI speaking\")\n}\n\nclass DebateTopic {\n  title string\n  description string\n}\n\nfunction PanelistA(topic: DebateTopic, debate_history: DebateMessage[]) -> string {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    You are Peter Thiel in a debate.\n    \n    Rules:\n    1. ONLY respond to the moderator's most recent question\n    2. Keep your response to exactly 1 sentence\n    3. Build on or challenge points from other panelists when relevant\n\n    Current debate topic:\n    Title: {{ topic.title }}\n    Description: {{ topic.description }}\n\n    Debate history:\n    {% for message in debate_history %}\n      {{ message.speaker_name }}: {{ message.content }}\n    {% endfor %}\n\n    Provide your response to the moderator's question in 1 sentences:\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction PanelistB(topic: DebateTopic, debate_history: DebateMessage[]) -> string {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    You are Eric Schmidt in a debate.\n    \n    Rules:\n    1. ONLY respond to the moderator's most recent question\n    2. Keep your response to exactly 1 sentence in simple english\n    3. Build on or challenge points from other panelists when relevant\n\n    Current debate topic:\n    Title: {{ topic.title }}\n    Description: {{ topic.description }}\n\n    Debate history:\n    {% for message in debate_history %}\n      {{ message.speaker_name }}: {{ message.content }}\n    {% endfor %}\n\n    Provide your response to the moderator's question in 1 sentences:\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction PanelistC(topic: DebateTopic, debate_history: DebateMessage[]) -> string {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    You are Sam Harris in a debate.\n    \n    Rules:\n    1. ONLY respond to the moderator's most recent question\n    2. Keep your response to exactly 1 sentence in simple english\n    3. Build on or challenge points from other panelists when relevant\n\n    Current debate topic:\n    Title: {{ topic.title }}\n    Description: {{ topic.description }}\n\n    Debate history:\n    {% for message in debate_history %}\n      {{ message.speaker_name }}: {{ message.content }}\n    {% endfor %}\n\n    Provide your response to the moderator's question in 1 sentence:\n    {{ ctx.output_format }}\n  \"#\n}\n\nfunction Moderator(topic: DebateTopic, debate_history: DebateMessage[]) -> string {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    You are the Joe Rogan, the Moderator of this debate.\n\n    Rules for different debate stages:\n    1. If this is the opening (no messages in history):\n       - Give a 1-2 sentence welcome\n       - Pose ONE clear, focused question to start the debate\n    2. After each round (after all panelists have spoken):\n       - Briefly highlight ONE key tension or interesting point\n       - Pose ONE new question that builds on the discussion\n    3. For the final round:\n       - Ask panelists for brief final thoughts\n    4. For closing:\n       - Provide a 1-2 sentence summary of key insights\n\n    Current debate topic:\n    Title: {{ topic.title }}\n    Description: {{ topic.description }}\n\n    Debate history:\n    {% for message in debate_history %}\n      {{ message.speaker_name }}: {{ message.content }}\n    {% endfor %}\n\n    {% if debate_history|length < 1 %}\n      Provide an opening welcome and initial question:\n    {% else %}\n      Provide your next moderation input following the rules above:\n    {% endif %}\n    {{ ctx.output_format }}\n  \"#\n}\n\ntest DebateTest {\n  functions [PanelistA, PanelistB, PanelistC, Moderator]\n  args {\n    topic {\n      title \"The Future of Artificial Intelligence\"\n      description \"Discussing the potential impacts, benefits, and risks of advanced AI systems\"\n    }\n    debate_history [\n      {\n        role \"moderator\"\n        speaker_name \"Moderator D\"\n        content \"Welcome to our panel discussion on the future of AI. Let's begin with Panelist A's opening statement.\"\n      }\n    ]\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.81.3\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
    "hikes.baml": "\nclass Hikes {\n  name string\n  location string\n  difficulty string\n  distance float\n  attractions string[]\n}\n\n\nfunction ExtractHikes(hikes: string) -> Hikes {\n  client \"openai/gpt-4o\"\n  prompt #\"\n    Extract from this content:\n    {{ hikes }}\n    {{ ctx.output_format }}\n  \"#\n}\n\ntest TestName {\n  functions [ExtractHikes]\n  args {\n    hikes #\"\nJust got back from an amazing hike at Mount Rainier National Park in Washington! The Skyline Trail Loop was absolutely worth the challenge. It's rated as moderate-difficult, mainly because of the 1,700 feet of elevation gain that definitely got my heart pumping. The loop is about 5.5 miles, and I spent a good 4 hours taking it all in.\n\nThe views were absolutely spectacular - so many wildflowers in bloom, and I could see Mount Adams, Mount St. Helens, and Mount Hood on the horizon. The trail starts at Paradise visitor center and takes you through alpine meadows with streams and waterfalls. Saw a family of marmots sunning themselves on the rocks! The Nisqually Glacier view point was definitely my favorite spot - truly breathtaking. Bring layers though, it gets chilly even in summer at those elevations!\n\n\n    \"#\n  }\n}\n",
    "resume.baml": "// Defining a data model.\nclass Resume {\n  name string\n  email string\n  experience string[]\n  skills string[]\n}\n\n// Create a function to extract the resume from a string.\nfunction ExtractResume(resume: string) -> Resume {\n  // Specify a client as provider/model-name\n  // you can use custom LLM params with a custom client name from clients.baml like \"client CustomHaiku\"\n  client \"openai/gpt-4o\" // Set OPENAI_API_KEY to use this client.\n  prompt #\"\n    Extract from this content:\n    {{ resume }}\n\n    {{ ctx.output_format }}\n  \"#\n}\n\n\n\n// Test the function with a sample resume. Open the VSCode playground to run this.\ntest vaibhav_resume {\n  functions [ExtractResume]\n  args {\n    resume #\"\n      Vaibhav Gupta\n      vbv@boundaryml.com\n\n      Experience:\n      - Founder at BoundaryML\n      - CV Engineer at Google\n      - CV Engineer at Microsoft\n\n      Skills:\n      - Rust\n      - C++\n    \"#\n  }\n}\n",
}

def get_baml_files():
    return file_map